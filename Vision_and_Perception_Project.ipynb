{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wp2lRDVUr79L"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0meW2Nk9IUtI"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import csv\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "\n",
        "from collections import Counter\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet152"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtQczdZbr024"
      },
      "source": [
        "# Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6fz5_Y_iSeYG"
      },
      "outputs": [],
      "source": [
        "class Dataset(Dataset):\n",
        "    def __init__(self, img_dir, transform=None):\n",
        "        super(Dataset, self).__init__()\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = [image for image in listdir(self.img_dir) if isfile(join(self.img_dir, image))]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_names[idx]\n",
        "        img = Image.open(join(self.img_dir, img_path)).convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return (img_path, img)\n",
        "\n",
        "\n",
        "class ResNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.model = resnet152()\n",
        "        self.model.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mwuy_L5GzGij"
      },
      "outputs": [],
      "source": [
        "src_dir_train = '/content/drive/MyDrive/VisualQuestionAnswering/scene_img_abstract_v002_train2015'\n",
        "dst_dir_train = '/content/drive/MyDrive/VisualQuestionAnswering/scene_img_feat_abstract_v002_train2015'\n",
        "\n",
        "resize_dim = 448\n",
        "batch_size = 6\n",
        "num_workers = 2\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize((resize_dim, resize_dim)), transforms.ToTensor()])\n",
        "\n",
        "dataset_train = Dataset(src_dir_train, transform=transform)\n",
        "\n",
        "loader_train = DataLoader(dataset_train, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "if not os.path.exists(dst_dir_train):\n",
        "    os.makedirs(dst_dir_train)\n",
        "  \n",
        "model = ResNet()\n",
        "\n",
        "for i, (img_paths, images) in enumerate(loader_train):\n",
        "    output = model(images)\n",
        "    if i == 0:\n",
        "        print(output.shape)\n",
        "\n",
        "    for j in range(len(img_paths)):\n",
        "        feat_name = img_paths[j].replace('.png', '.npy')\n",
        "        feat_name = join(dst_dir_train, feat_name)\n",
        "        print(feat_name)\n",
        "        np.save(feat_name, output[j].data.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6rQ4vWQPF7C-"
      },
      "outputs": [],
      "source": [
        "src_dir_val = '/content/drive/MyDrive/VisualQuestionAnswering/scene_img_abstract_v002_val2015'\n",
        "dst_dir_val = '/content/drive/MyDrive/VisualQuestionAnswering/scene_img_feat_abstract_v002_val2015'\n",
        "\n",
        "dataset_val = Dataset(src_dir_val, transform=transform)\n",
        "\n",
        "loader_val = DataLoader(dataset_val, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n",
        "\n",
        "if not os.path.exists(dst_dir_val):\n",
        "    os.makedirs(dst_dir_val)\n",
        "\n",
        "for i, (img_paths, images) in enumerate(loader_val):\n",
        "    output = model(images)\n",
        "    if i == 0:\n",
        "        print(output.shape)\n",
        "\n",
        "    for j in range(len(img_paths)):\n",
        "        feat_name = img_paths[j].replace('.png', '.npy')\n",
        "        feat_name = join(dst_dir_val, feat_name)\n",
        "        print(feat_name)\n",
        "        np.save(feat_name, output[j].data.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JDFGhrXrdMf"
      },
      "source": [
        "# Questions and Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "og7oNHiFmeyd"
      },
      "outputs": [],
      "source": [
        "# Interface for accessing the VQA dataset.\n",
        "\n",
        "# This code is based on the code available at the following link: https://github.com/GT-Vision-Lab/VQA/blob/master/PythonHelperTools/vqaTools/vqa.py.\n",
        "\n",
        "\n",
        "class VQA:\n",
        "\n",
        "    def __init__(self, annotation_file=None, question_file=None):\n",
        "        \"\"\" Constructor of VQA class for reading and visualizing questions and answers. \"\"\"\n",
        "        # load dataset\n",
        "        self.dataset = {}\n",
        "        self.questions = {}\n",
        "        self.qa = {}\n",
        "        self.qqa = {}\n",
        "        self.imgToQA = {}\n",
        "        if annotation_file is not None and question_file is not None:\n",
        "            dataset = json.load(open(annotation_file, 'r'))\n",
        "            questions = json.load(open(question_file, 'r'))\n",
        "            self.dataset = dataset\n",
        "            self.questions = questions\n",
        "            self.createIndex()\n",
        "\n",
        "    def createIndex(self):\n",
        "\n",
        "        # create index\n",
        "        imgToQA = {ann['image_id']: [] for ann in self.dataset['annotations']}\n",
        "        qa = {ann['question_id']: [] for ann in self.dataset['annotations']}\n",
        "        qqa = {ann['question_id']: [] for ann in self.dataset['annotations']}\n",
        "        for ann in self.dataset['annotations']:\n",
        "            imgToQA[ann['image_id']] += [ann]\n",
        "            qa[ann['question_id']] = ann\n",
        "        for ques in self.questions['questions']:\n",
        "            qqa[ques['question_id']] = ques\n",
        "\n",
        "        # create class members\n",
        "        self.qa = qa\n",
        "        self.qqa = qqa\n",
        "        self.imgToQA = imgToQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EWmqKSzv-wxS"
      },
      "outputs": [],
      "source": [
        "manualMap = {'none': '0', 'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10'}\n",
        "\n",
        "articles = ['a', 'an', 'the']\n",
        "\n",
        "\n",
        "def processDigitArticle(inText):\n",
        "\toutText = []\n",
        "\ttempText = inText.lower().split()  # Making all characters lowercase.\n",
        "\tfor word in tempText:\n",
        "\t\tword = manualMap.setdefault(word, word)  # Converting number words to digits.\n",
        "\t\t# The setdefault() method returns the value of the item with the specified key. If the key does not exist, insert the key, with the specified value.\n",
        "\t\tif word not in articles:\n",
        "\t\t\toutText.append(word)  # Removing articles (a, an, the).\n",
        "\t\telse:\n",
        "\t\t\tpass\n",
        "\toutText = ' '.join(outText)\n",
        "\t# The join() method takes all items in an iterable and joins them into one string. ' ' is the separator.\n",
        "\treturn outText\n",
        "\n",
        "\n",
        "punct = [';', \"/\", '[', ']', '\"', '{', '}', '(', ')', '=', '+', '\\\\', '_', '-', '>', '<', '@', '`', ',', '?', '!']  \n",
        "\n",
        "\n",
        "def processPunctuation(inText):\n",
        "\toutText = inText\n",
        "\tfor p in punct:  # Replacing all punctuation with a space character.\n",
        "\t\tif (p + ' ' in inText or ' ' + p in inText):  \n",
        "\t\t\toutText = outText.replace(p, '')\n",
        "\t\telse:\n",
        "\t\t\toutText = outText.replace(p, ' ')\n",
        "\treturn outText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "X97alyRomP7p"
      },
      "outputs": [],
      "source": [
        "# This code is inspired by the code available at the following link: https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py.\n",
        "\n",
        "\n",
        "class VqaTrainDataset(Dataset):\n",
        "\n",
        "    \"\"\" Load the VQA dataset using the VQA class. \"\"\"\n",
        "\n",
        "    def __init__(self, question_json_file_path, annotation_json_file_path, image_filename_pattern, img_features_dir, vocab_json_filename):\n",
        "          \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            question_json_file_path (string): Path to the json file containing the questions\n",
        "            annotation_json_file_path (string): Path to the json file containing the annotations\n",
        "            image_filename_pattern (string): Pattern used by the filenames of the images in this dataset (eg \"abstract_v002_train2015_{}.png\")\n",
        "            img_features_dir (string): Path to the directory with image features\n",
        "            vocab_json_filename (string): Path to the vocabulary.\n",
        "        \"\"\"\n",
        "\n",
        "        vqa_db = VQA(annotation_file=annotation_json_file_path, question_file=question_json_file_path)\n",
        "\n",
        "        self.max_words_in_ques = -1  \n",
        "        self.dataset = []\n",
        "\n",
        "        ques_list = []\n",
        "        ans_list = []\n",
        "\n",
        "        for q_id, annotation in vqa_db.qa.items():\n",
        "            entry = {}\n",
        "            question = vqa_db.qqa[q_id]['question']\n",
        "            question = processPunctuation(question)\n",
        "            question = processDigitArticle(question)\n",
        "            words = question.split(' ')\n",
        "            if len(words) > self.max_words_in_ques:\n",
        "                self.max_words_in_ques = len(words)\n",
        "            ques_list += words\n",
        "            entry['ques'] = words\n",
        "            answer_objs = annotation['answers']\n",
        "\n",
        "            possible_answers = [a['answer'] for a in answer_objs]\n",
        "\n",
        "            entry['possible_answers'] = []\n",
        "            for answer in possible_answers:\n",
        "                mod_ans = processPunctuation(answer)\n",
        "                mod_ans = processDigitArticle(mod_ans)\n",
        "                ans_list.append(mod_ans)\n",
        "                entry['possible_answers'].append(mod_ans)\n",
        "      \n",
        "            img_full_idx = \"%012d\" % annotation['image_id']  # '00000000000image_id'\n",
        "            img_name = image_filename_pattern.replace('{}', img_full_idx)\n",
        "            img_feature_loc = os.path.join(img_features_dir, img_name.replace('.png', '.npy'))\n",
        "            entry['img_feat_loc'] = img_feature_loc\n",
        "\n",
        "            self.dataset.append(entry)\n",
        "\n",
        "        q_vocab = self.build_vocab(ques_list)\n",
        "        a_vocab = self.build_vocab(ans_list)\n",
        "        vocab = {'q': q_vocab, 'a': a_vocab}\n",
        "\n",
        "        f = open(vocab_path, \"w\")\n",
        "        json.dump(vocab, f)\n",
        "        f.close()\n",
        "        \n",
        "        self.q_vocab = vocab['q']\n",
        "        self.q_vocab_size = len(self.q_vocab.keys())\n",
        "            \n",
        "        self.a_vocab = vocab['a']\n",
        "        self.a_vocab_size = len(self.a_vocab.keys())\n",
        "\n",
        "    def build_vocab(self, data):\n",
        "        counter = Counter(data)\n",
        "        words = counter.keys()\n",
        "        tokens = sorted(words, key=lambda x: (counter[x], x), reverse=True)  # reverse=True: sorts in a descending order.\n",
        "        vocab = {t: i for i, t in enumerate(tokens)}\n",
        "        return vocab\n",
        "\n",
        "    def _get_q_encoding(self, questions):\n",
        "        vec = torch.zeros(self.q_vocab_size)\n",
        "        for question in questions:\n",
        "            if question in self.q_vocab:\n",
        "                vec[self.q_vocab[question]] += 1\n",
        "        return vec, len(questions)\n",
        "\n",
        "    def _get_a_encoding(self, answers):\n",
        "        vec = torch.zeros(self.a_vocab_size)\n",
        "        for answer in answers:\n",
        "            if answer in self.a_vocab:\n",
        "                vec[self.a_vocab[answer]] += 1\n",
        "        return vec\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.dataset[idx]\n",
        "\n",
        "        image_encoding = np.load(entry['img_feat_loc'])\n",
        "\n",
        "        ques = entry['ques']\n",
        "        ques_encoding, ques_len = self._get_q_encoding(ques)\n",
        "\n",
        "        possible_answers = entry['possible_answers']\n",
        "        ans_encoding = self._get_a_encoding(possible_answers)\n",
        "\n",
        "        return {'image_enc': image_encoding, 'ques_enc': ques_encoding, 'ques_len': ques_len, 'ans_enc': ans_encoding}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "K0rLM2xhNDYP"
      },
      "outputs": [],
      "source": [
        "# This code is inspired by the code available at the following link: https://github.com/Cyanogenoid/pytorch-vqa/blob/master/data.py.\n",
        "\n",
        "\n",
        "class VqaValDataset(Dataset):\n",
        "\n",
        "    \"\"\"\n",
        "    Load the VQA dataset using the VQA class.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, question_json_file_path, annotation_json_file_path, image_filename_pattern, img_features_dir, vocab_json_filename):\n",
        "                 \n",
        "        \"\"\"\n",
        "        Args:\n",
        "            question_json_file_path (string): Path to the json file containing the questions\n",
        "            annotation_json_file_path (string): Path to the json file containing the annotations\n",
        "            image_filename_pattern (string): Pattern used by the filenames of the images in this dataset (eg \"abstract_v002_val2015_{}.png\")\n",
        "            img_features_dir (string): Path to the directory with image features\n",
        "            vocab_json_filename (string): Path to the vocabulary.\n",
        "        \"\"\"\n",
        "\n",
        "        vqa_db = VQA(annotation_file=annotation_json_file_path, question_file=question_json_file_path)\n",
        "\n",
        "        self.max_words_in_ques = -1\n",
        "            \n",
        "        self.dataset = []\n",
        "\n",
        "        for q_id, annotation in vqa_db.qa.items():\n",
        "            entry = {}\n",
        "            question = vqa_db.qqa[q_id]['question']\n",
        "            question = processPunctuation(question)\n",
        "            question = processDigitArticle(question)\n",
        "            words = question.split(' ')\n",
        "            if len(words) > self.max_words_in_ques:\n",
        "                self.max_words_in_ques = len(words)\n",
        "            entry['ques'] = words\n",
        "            answer_objs = annotation['answers']\n",
        "\n",
        "            possible_answers = [a['answer'] for a in answer_objs]\n",
        "\n",
        "            entry['possible_answers'] = []\n",
        "            for answer in possible_answers:\n",
        "                mod_ans = processPunctuation(answer)\n",
        "                mod_ans = processDigitArticle(mod_ans)\n",
        "                entry['possible_answers'].append(mod_ans)\n",
        "                \n",
        "            img_full_idx = \"%012d\" % annotation['image_id']  # '00000000000image_id'\n",
        "            img_name = image_filename_pattern.replace('{}', img_full_idx)\n",
        "            img_feature_loc = os.path.join(img_features_dir, img_name.replace('.png', '.npy'))\n",
        "            entry['img_feat_loc'] = img_feature_loc\n",
        "            \n",
        "            self.dataset.append(entry)\n",
        "\n",
        "        vocab = json.load(open(vocab_json_filename, \"r\"))\n",
        "\n",
        "        self.q_vocab = vocab['q']\n",
        "        self.q_vocab_size = len(self.q_vocab.keys())\n",
        "            \n",
        "        self.a_vocab = vocab['a']\n",
        "        self.a_vocab_size = len(self.a_vocab.keys())\n",
        "\n",
        "    def _get_q_encoding(self, questions):\n",
        "        vec = torch.zeros(self.q_vocab_size)\n",
        "        for question in questions:\n",
        "            if question in self.q_vocab:\n",
        "                vec[self.q_vocab[question]] += 1\n",
        "        return vec, len(questions)\n",
        "\n",
        "    def _get_a_encoding(self, answers):\n",
        "        vec = torch.zeros(self.a_vocab_size)\n",
        "        for answer in answers:\n",
        "            if answer in self.a_vocab:\n",
        "                vec[self.a_vocab[answer]] += 1\n",
        "        return vec\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        entry = self.dataset[idx]\n",
        "\n",
        "        image_encoding = np.load(entry['img_feat_loc'])\n",
        "\n",
        "        ques = entry['ques']\n",
        "        ques_encoding, ques_len = self._get_q_encoding(ques)\n",
        "\n",
        "        possible_answers = entry['possible_answers']\n",
        "        ans_encoding = self._get_a_encoding(possible_answers)\n",
        "\n",
        "        return {'image_enc': image_encoding, 'ques_enc': ques_encoding, 'ques_len': ques_len, 'ans_enc': ans_encoding}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFrpkrT7qJOF"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjg3rh0moUII"
      },
      "source": [
        "## SimpleBaselineNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rtEjHwt3nlYs"
      },
      "outputs": [],
      "source": [
        "class SimpleBaselineNet(nn.Module):\n",
        "\n",
        "    \"\"\" Predicts an answer to a question about an image using the Simple Baseline for Visual Question Answering (Zhou et al, 2017) paper. \"\"\"\n",
        "    \n",
        "    def __init__(self, img_feat_size, q_vocab_size, a_vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_feat_size = img_feat_size\n",
        "        self.q_vocab_size = q_vocab_size\n",
        "        self.a_vocab_size = a_vocab_size\n",
        "        self.q_embedding_size = 1024\n",
        "\n",
        "        self.linear_layer = nn.Linear(self.q_vocab_size, self.q_embedding_size, bias=False)  # Applies a linear transformation to the incoming data: y = xA^T + b.\n",
        "        self.classifier = nn.Linear(self.img_feat_size + self.q_embedding_size, self.a_vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, image_encoding, question_encoding):\n",
        "\n",
        "        image_encoding = image_encoding.view(image_encoding.shape[0], -1)  # image_encoding.size(): torch.Size([24, 1000]).\n",
        "\n",
        "        question_embedding = self.linear_layer(question_encoding)\n",
        "\n",
        "        x = torch.cat((image_encoding, question_embedding), dim=-1)  # Concatenates the given sequence of tensors (image_encoding, question_embedding) in the given dimension (dim=-1).\n",
        "\n",
        "        out = self.classifier(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chY8SLwLn_M5"
      },
      "source": [
        "## My Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZkkHsbtP0Jke"
      },
      "outputs": [],
      "source": [
        "class QuestionProcessor(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size):\n",
        "        super(QuestionProcessor, self).__init__()\n",
        "\n",
        "        self.word_embedding = 300\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, self.word_embedding, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=self.word_embedding, hidden_size=embedding_size, num_layers=2)   \n",
        "\n",
        "    def forward(self, encoding, length):      \n",
        "        embedding = self.embedding(encoding.long())  # long() converts a torch.FloatTensor to a torch.LongTensor.\n",
        "                \n",
        "        embedding = nn.utils.rnn.pack_padded_sequence(embedding, length, batch_first=True, enforce_sorted=False)  # nn.utils.rnn.pack_padded_sequence() packs a Tensor containing padded sequences of variable length.\n",
        "        # enforce_sorted=False: the input will get sorted unconditionally.\n",
        "\n",
        "        _, out = self.lstm(embedding)\n",
        "        return out[0][0]  # return the final hidden state of LSTM.\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, img_feat_size, q_vocab_size, a_vocab_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.img_feat_size = img_feat_size\n",
        "        self.q_vocab_size = q_vocab_size\n",
        "        self.a_vocab_size = a_vocab_size\n",
        "        self.q_embedding_size = 1024\n",
        "\n",
        "        self.question_processor = QuestionProcessor(self.q_vocab_size, self.q_embedding_size)\n",
        "\n",
        "        self.linear_1 = nn.Linear(self.img_feat_size + self.q_embedding_size, self.img_feat_size, bias=False)\n",
        "        self.linear_2 = nn.Linear(self.img_feat_size, self.a_vocab_size, bias=False)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        \n",
        "    def forward(self, image_encoding, question_encoding, question_length):\n",
        "      \n",
        "        image_encoding = image_encoding.view(image_encoding.shape[0], -1)  # image_encoding.size(): torch.Size([_, 1000]).\n",
        "        question_embedding = self.question_processor(question_encoding, question_length)  # question_embedding.size(): torch.Size([_, 1024])\n",
        "\n",
        "        x = torch.cat((image_encoding, question_embedding), dim=-1)  # Concatenates the given sequence of tensors (image_encoding, question_embedding) in the given dimension (dim=-1).\n",
        "\n",
        "        out = self.linear_2(self.activation(self.linear_1(x)))\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTh28ay3pA8N"
      },
      "source": [
        "# Train and Validate "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1LQg_4hXnv5p"
      },
      "outputs": [],
      "source": [
        "train_question_path = '/content/drive/MyDrive/VisualQuestionAnswering/OpenEnded_abstract_train2015_questions.json'\n",
        "train_annotation_path = '/content/drive/MyDrive/VisualQuestionAnswering/abstract_train2015_annotations.json'\n",
        "train_img_feat_path = '/content/drive/MyDrive/VisualQuestionAnswering/scene_img_feat_abstract_v002_train2015'\n",
        "\n",
        "val_question_path = '/content/drive/MyDrive/VisualQuestionAnswering/OpenEnded_abstract_val2015_questions.json'\n",
        "val_annotation_path = '/content/drive/MyDrive/VisualQuestionAnswering/abstract_val2015_annotations.json'\n",
        "val_img_feat_path = '/content/drive/MyDrive/VisualQuestionAnswering/scene_img_feat_abstract_v002_val2015'\n",
        "\n",
        "vocab_path = '/content/drive/MyDrive/VisualQuestionAnswering/vocab.json'\n",
        "\n",
        "# results = '/content/drive/MyDrive/VisualQuestionAnswering/SimpleBaselineNet.txt'\n",
        "results = '/content/drive/MyDrive/VisualQuestionAnswering/MyNet.txt'\n",
        "\n",
        "num_epochs = 15\n",
        "\n",
        "batch_size = 6\n",
        "num_data_loader_workers = 2\n",
        "\n",
        "img_feat_size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lFpaARiguv6n"
      },
      "outputs": [],
      "source": [
        "def optimize(criterion, predicted_answer, optimizer, ground_truth_answer):\n",
        "\n",
        "    majority_ans = torch.argmax(ground_truth_answer, dim=-1)\n",
        "    loss = criterion(predicted_answer, majority_ans)  # loss = criterion(output_model, target)\n",
        "    optimizer.zero_grad()  # zero_grad(): zeroes the grad attribute of all the parameters passed to the optimizer.\n",
        "    loss.backward()  # backward(): performs the gradient of all the parameters for which require_grad = True and stores the gradient in parameter.grad attribute for every parameter.\n",
        "    optimizer.step()  # step(): updates the value of all the parameters passed to the optimizer (based on parameter.grad).\n",
        "    return loss\n",
        "\n",
        "\n",
        "def validate(model, val_dataset_loader):\n",
        "\n",
        "    correct_answers = 0\n",
        "    total_answers = 0\n",
        "\n",
        "    for batch_id, batch_data in enumerate(val_dataset_loader):\n",
        "        image_encoding = batch_data['image_enc']\n",
        "        question_encoding = batch_data['ques_enc']\n",
        "        question_length = batch_data['ques_len']\n",
        "        ground_truth_answer = batch_data['ans_enc']\n",
        "\n",
        "        batch_size = ground_truth_answer.shape[0]\n",
        "\n",
        "        logits = model(image_encoding, question_encoding, question_length)\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        predicted_answer = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        counts = ground_truth_answer[torch.arange(batch_size), predicted_answer]  # torch.arange(): returns a 1-D tensor with values in the range [start, end) with start = 0 and end = batch_size.\n",
        "        correct_answers = correct_answers + float(torch.sum(torch.min(counts/3, torch.ones(1))))\n",
        "            \n",
        "        total_answers = total_answers + batch_size\n",
        "\n",
        "    return (correct_answers / total_answers) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UofRKAAglppf"
      },
      "outputs": [],
      "source": [
        "train_dataset = VqaTrainDataset(question_json_file_path=train_question_path,\n",
        "                                annotation_json_file_path=train_annotation_path,\n",
        "                                image_filename_pattern=\"abstract_v002_train2015_{}.png\",\n",
        "                                img_features_dir=train_img_feat_path,\n",
        "                                vocab_json_filename=vocab_path)\n",
        "\n",
        "val_dataset = VqaValDataset(question_json_file_path=val_question_path,\n",
        "                            annotation_json_file_path=val_annotation_path,\n",
        "                            image_filename_pattern=\"abstract_v002_val2015_{}.png\",\n",
        "                            img_features_dir=val_img_feat_path,\n",
        "                            vocab_json_filename=vocab_path)\n",
        "\n",
        "\n",
        "train_dataset_loader = DataLoader(train_dataset,\n",
        "                                  batch_size=batch_size,  # batch_size: represents how many samples per batch to load.\n",
        "                                  shuffle=True,\n",
        "                                  num_workers=num_data_loader_workers)  # num_workers: represents how many subprocesses to use for loading data.\n",
        "\n",
        "val_dataset_loader = DataLoader(val_dataset,\n",
        "                                batch_size=batch_size,\n",
        "                                shuffle=False,\n",
        "                                num_workers=num_data_loader_workers)\n",
        "\n",
        "\n",
        "q_vocab_size = train_dataset.q_vocab_size\n",
        "a_vocab_size = train_dataset.a_vocab_size\n",
        "\n",
        "# model = SimpleBaselineNet(img_feat_size, q_vocab_size, a_vocab_size)\n",
        "model = MyNet(img_feat_size, q_vocab_size, a_vocab_size)\n",
        "\n",
        "out_filename = open(results, \"w\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    num_batches = len(train_dataset_loader)\n",
        "\n",
        "    for batch_id, batch_data in enumerate(train_dataset_loader):\n",
        "        model.train()  # Set the model to train mode\n",
        "\n",
        "        image_encoding = batch_data['image_enc']\n",
        "        question_encoding = batch_data['ques_enc']\n",
        "        question_length = batch_data['ques_len']\n",
        "        ground_truth_answer = batch_data['ans_enc']\n",
        "\n",
        "        predicted_answer = model(image_encoding, question_encoding, question_length)\n",
        "        \n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        # optimizer = torch.optim.SGD([{'params': model.fc_ques.parameters(), 'lr': 0.8}, {'params': model.classifier.parameters(), 'lr': 0.01}])  # for SimpleBaselineNet \n",
        "        optimizer = torch.optim.SGD([{'params': model.linear_1.parameters(), 'lr': 0.01}, {'params': model.linear_2.parameters(), 'lr': 0.01}])\n",
        "\n",
        "        loss = optimize(criterion, predicted_answer, optimizer, ground_truth_answer)\n",
        "\n",
        "        if batch_id == (num_batches - 1):\n",
        "            model.eval()  # Set the model to eval mode\n",
        "            val_accuracy = validate(model, val_dataset_loader)\n",
        "            print(\"Epoch {} has val accuracy: {}\".format(epoch, val_accuracy))\n",
        "            out_filename.write(\"Epoch {} has val accuracy: {}\\n\".format(epoch, val_accuracy))\n",
        "\n",
        "\n",
        "out_filename.close() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icxI2UysObMc"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(20, 15))\n",
        "\n",
        "# Simple Baseline Net\n",
        "plt.subplot(2, 2, 1)\n",
        "epoch_list_SimpleBaselineNet = []\n",
        "accuracy_list_SimpleBaselineNet = []\n",
        "\n",
        "with open('/content/drive/MyDrive/VisualQuestionAnswering/SimpleBaselineNet.txt', 'r') as f:\n",
        "    data = f.read().split(\"\\n\")\n",
        "for i in range(len(data)-1):\n",
        "    epoch_list_SimpleBaselineNet.append(int(data[i].split(\" \")[1]))\n",
        "    accuracy_list_SimpleBaselineNet.append(float(data[i].split(\" \")[5]))\n",
        "\n",
        "plt.plot(epoch_list_SimpleBaselineNet, accuracy_list_SimpleBaselineNet, label=\"Simple Baseline Net\", color='blue', linewidth=3.0)\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "plt.legend(loc='lower right', prop={'size': 15})\n",
        "plt.savefig('/content/drive/MyDrive/VisualQuestionAnswering/SimpleBaselineNet.png')\n",
        "\n",
        "\n",
        "# My Net\n",
        "plt.subplot(2, 2, 2)\n",
        "epoch_list_MyNet = []\n",
        "accuracy_list_MyNet = []\n",
        "\n",
        "with open('/content/drive/MyDrive/VisualQuestionAnswering/MyNet.txt', 'r') as f:\n",
        "    data = f.read().split(\"\\n\")\n",
        "for i in range(len(data)-1):\n",
        "    epoch_list_MyNet.append(int(data[i].split(\" \")[1]))\n",
        "    accuracy_list_MyNet.append(float(data[i].split(\" \")[5]))\n",
        "\n",
        "plt.plot(epoch_list_MyNet, accuracy_list_MyNet, label=\"My Net\", color='red', linewidth=3.0)\n",
        "plt.xlabel('Epochs', fontsize=15)\n",
        "plt.ylabel('Accuracy', fontsize=15)\n",
        "plt.legend(loc='lower right', prop={'size': 15})\n",
        "plt.savefig('/content/drive/MyDrive/VisualQuestionAnswering/MyNet.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-j_sRPF4tvY"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({\"Net\": [\"Simple Baseline Net\", \"My Net\"],\n",
        "                   \"Epochs\": [epoch_list_SimpleBaselineNet[-1]+1, epoch_list_MyNet[-1]+1],\n",
        "                   \"Accuracy\": [accuracy_list_SimpleBaselineNet[-1], accuracy_list_MyNet[-1]]})\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/EasyVisualQuestionAnswering/results.csv', index=False, encoding='utf-8')\n",
        "\n",
        "print(df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}